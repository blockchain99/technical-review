Question 1 :
1. I  compared the same size of sorted substring of 1st string with 2nd sorted string : 
def Qusetion1(s,t): 
       for in in range(len(s) – len(t) +1): 
       if there is the same sorted substring in 1st string with 2nd 
       sorted string, return True, else stay False
2. Time efficiency approximation :
-In worst case scenario, the length of second string, len(t)=1,So, for-loop is executed number of time, 
 length of 1st input string,s (len(s)-1 +1 = len(s))
-Average case : (len(s) – len(s/2) +1)
-Best case : (len(s) – len(s) +1) = 1 
3. Approximation of time efficiency : O(n) for the line "for i in range(s_length - t +1)", 
   O(n log n), n is the length of s : for the line "if sorted(s[i: i+tLlength]) == sorted(t)".
   So, overall runtime time complixty is O( n log n)

Question 2 :
1. Given a string a, find the longest palindromic substring contained in a.
   def is_palindrome(a)      
       return str(a) == str(a)[::-1]
   while i < (len(a) - len(longest_palindrome)):
        .. is_palindrome(temp_string)==True:
2.  As for input string  such as “reacecar”  : len(‘racecar’)= 7, there are 7time comparison and   
replacing palindrome with  the increased temp string . Also, it took extra space to store 
temp_string and replace palindrome.(So, space efficiency is lower than others)
As for input string "cumquat"  : it compared 7 times but did not replace it with palindrome .
3.Approximation of time efficiency is O(N*N) : above while.. line run at O(n) and at each iteration run 
  str(a) == str(a)[::-1]which run at O(N). So Approximation of time complxity is O(N*N) 
   

Question 3 :
1 I’d find the Minimum spanning tree using Kruskal’s algorithm as follows.
def question3(graph):  
    for vertice in graph['vertices']:
        make_set(vertice)       
    for edge in edges:
       if find(vertice1) != find(vertice2):
As for input testgraph1 = {'A':[('B',1),('C',5),('D',3)],'B':[('C',4),('D',2)],'C':[('D',1)]}, 
which converted to dictionary {'vertices': ['A', 'B', 'C', 'D', 'E', 'F'],
    'edges': set([ (1, 'A', 'B'),(5, 'A', 'C'), (3, 'A', 'D'),(4, 'B', 'C'), (2, 'B', 'D'), (1, 'C', 'D')])}. 
In this dictionary  data structure, keys are the vertices and the values are the weights(ranks)
And key steps and pseudo code are as follows.
  - Sort all the edges in non-decreasing order of their weight.
  - Pick the smallest edge. Check if it forms a cycle with the spanning tree 
    formed so far. If cycle is not formed, include this edge. Else, discard it.  
  - Repeat step#2 until there are (V-1) edges in the spanning tree.

KRUSKAL(G):                              #Time efficiency 
1 A = ?                            - -       # O(1)
2 foreach v ? G.V:            --  \
                                    -  # O(V)
3    MAKE-SET(v)               - -/
4 sort the edges of G.E into non decreasing order by weight w                        - # O(E logE)     
5 foreach (u, v) in G.E ordered by weight(u, v), increasing:                       \
6    if FIND-SET(u) ? FIND-SET(v):                                                  \  
7       A = A ? {(u, v)}                                                              -   # O(V logV)
8       UNION(u, v)                                                                 / 
9 return                                                                           /  
--------------------------------------------------------------
Complexity of kruskal algorithm:
T(n) = O(1) + O(V) + O(E log E) + O(V log V)
     = O(E log E) + O(V log V)
as |E| >= |V| - 1
T(n) = E log E + E log E
     = E log E
Where E is the number of edges in the graph and V is the number of vertices, 
Kruskal's algorithm can be shown to run in O(E log E) time, or equivalently,
 O(E log V) time, all with simple data structures. These running times are equivalent because:
E is at most V2 and {\displaystyle \log V^{2}=2\log V} {\displaystyle \;} is O(log V).
Each isolated vertex is a separate component of the minimum spanning forest.
 If we ignore isolated vertices we obtain V ? 2E, so log V is O(log E).
We can achieve this bound as follows: first sort the edges by weight 
using a comparison sort in O(E log E) time; this allows the step "remove 
an edge with minimum weight from S" to operate in constant time. 
Next, we use a disjoint-set data structure (Union&Find) to keep track of
 which vertices are in which components. We need to perform O(V) operations, 
as in each iteration we connect a vertex to the spanning tree, two 'find' operations 
and possibly one union for each edge. Even a simple disjoint-set data structure 
such as disjoint-set forests with union by rank can perform O(V) operations in O(V log V) time.
 Thus the total time is O(E log E) = O(E log V).

2.As for the conversion given format to dictionary consists of  key ‘vertice’ and
 it’s value list and key ‘edges’ and it’s value set as follows.
Given format : testgraph1 = {'A':[('B',1),('C',5),('D',3)],'B':[('C',4),('D',2)],'C':[('D',1)]}
Converted dictionary : graph1 = {'vertices': ['A', 'B', 'C', 'D', 'E', 'F'],
'edges': set([ (1, 'A', 'B'),(5, 'A', 'C'), (3, 'A', 'D'),(4, 'B', 'C'), (2, 'B', 'D'), (1, 'C', 'D')])}
 It  took “number of key * number of value * number of each element”  in dictionary  to execute 
 in function  def make_edges_list(testgraph):
                               
         
Question 4 :

I implemented this function to find the least common ancestor by checking balanced tree. 

1) Time complexity : bast case scenarion 

As for question4([[0, 1, 0, 0, 0],
[0, 0, 0, 0, 0],
[0, 0, 0, 0, 0],
[1, 0, 0, 0, 1],
[0, 0, 0, 0, 0]],
3,
1,
4)
      
 Index0    i1    i2    i3    i4
Node 0 :  0    1     0     0     0
Node 1 :  0    0     0     0     0
Node 2 :  0    0     0     0     0 
Node 3 :  1    0     0     0     1
Node 4 :  0    0     0     0     0
            (3):root                (2)
          /          \      
       (0)          (4):n2
          \     
          (1):n1

 

I used the definition of BST, and itrative approach with Node class, which reduce recursive calls. 
  So, space complixty is O(1).
question4([[0, 1, 0, 0, 0],
[0, 0, 0, 0, 0],
[0, 0, 0, 0, 0],
[1, 0, 0, 0, 1],
[0, 0, 0, 0, 0]],
3,
1,
4)
      
     Index0    i1    i2    i3    i4
Node 0 :  0    1     0     0     0
Node 1 :  0    0     0     0     0
Node 2 :  0    0     0     0     0 
Node 3 :  1    0     0     0     1
Node 4 :  0    0     0     0     0
            (3):root                (2)
          /          \      
       (0)          (4):n2
          \     
          (1):n1
  2) worst case secnario : unbalnced tree , time complexity is O(N)
     such as belows.
Question4([[0,0,0,0,0]
[1,0,0,0,0]
[0,1,0,0,0]
[0,1,0,0,0]
[0,0,1,0,0]
[0,0,0,1,0]],
6,
0,
1)
       
            (4)
            /
          (3)
          /
        (2)
        /
      (1)  
      /
    (0)
- Right child tree and balnced tree test result is include in the code.
- Most operations on a binary search tree (BST) take time directly proportional to the height of the tree
  and it follows that for a tree with n nodes and height h:
  h >= log2(n+1) -1 >= log2N (*2 is base of log), so the minimun height of a tree with n nodes is log2n(*2 is 
  base of log) 
 - A self-balancing BST structure containing n items allows the lookup, insertion, and removal of
   an item in O(log n) worst-case time, and ordered enumeration of all items in O(n) time. (by wikipedia)
- And height of a complete balanced tree with node n is  log(n+1) 
- Approximation of Time complixity is O(N * log(n)) in best O(N*N) in worst case. 
- It can be solved with constant space, O(1)


Question5:

In question5(ll, m), where ll is the first node of a linked list and m is the "mth number from the end,
 so , m is the "m-th number from the end, which is  equal element with (linked_list size+1)-m from start.

        while ll :  #while current exit(not None)
            if counter == (l_size+1)-m:         

 I used linked list , which has following Time efficiency :
________________________________________________________________________________________
                          Insert            Delete           search        Space Usage
Unsorted linked list     | O(1)*        | O(1)*          | O(n)           | O(n)   
----------------------------------------------------------------------------------------

*) The cost to add or delete an element into a known location in the list 
(i.e. if I have an iterator to the location) is O(1). If I don't know the location, 
then I need to traverse the list to the location of deletion/insertion, which takes O(n) time.
 The time complexity for the Inserting at the end depends if I have the location of the last node, 
if I do, it would be O(1) other wise I will have to search through the linked list 
and the time complexity would jump to O(n).

class LinkedList(object):
    def __init__(self, head=None):
        self.head = head                                                   # O(1)
# Insert at the end of linked list                                         # O(1)
# otherwise, I have to search through the lined list                       # O(N)
    def append(self, new_element):                         
        current = self.head                                           
        if self.head:  #if head is exist(not None)
            while current.next:   #and if current's next element exist
                current = current.next     #shift to next 
            current.next = new_element   #current's next element become new_element.   
        else:
            self.head = new_element
            
#calculate the  size of linked  list    
    def linked_list_size(self):                                            # O(N)
        temp=self.head
        count=0
        while(temp):
            count+=1
            temp=temp.next
        return count

# Assume the first position is 1.
# Return "None" if position is not in the list.
    def question5(self,ll,m):    
        counter = 1
        ll = self.head
        l_size = self.linked_list_size()
        if m < 1:   # None for the first position less than  0
            return None
        # m is the "mth number from the end, which is  equal element with (linked_list size+1)-m from start.
        while ll :  #while current exit(not None)                    
            if counter == (l_size+1)-m:                       
                return ll.data
            ll = ll.next                                          # O(N)
            counter += 1
        return None
 For example,  linked list size is 5 and last 2th number from the end, it makes  5+1 -2 = 4,
which  is 4th linked list element from the start, so If you insert this 4th linked list element,
we need to 4 times operation of  linked list.

#create Node with 1,2,3,4,5      # Time complexity 
e1 = Node(1)                            
e2 = Node(2)                              
e3 = Node(3)                                
e4 = Node(4)
e5 = Node(5)
# Start setting up a LinkedList
ll = LinkedList(e1)            #O(1)
ll.append(e2)                  #O(1)
ll.append(e3)                  #O(1)   
ll.append(e4)                  #O(1)
ll.append(e5)                  #O(1)

The time complexity for the Inserting at the end depends if I have the location of the last node, 
it would be O(1), otherwise, I  need to search through the linked list 
and the time complexity would jump to O(n).
In this case, the approximate time efficiency is O(N).

